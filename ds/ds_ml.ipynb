{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231693d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83f378",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfff045",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9ad9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CSV_FILE_PATH = '../scraper/data/crypto_data.csv'\n",
    "\n",
    "print(\"Loading data from CSV...\")\n",
    "df = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93f6fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"INITIAL DATA INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Basic info\n",
    "print(\"\\n--- Dataset Info ---\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n--- Data Types ---\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n--- Dataset Shape ---\")\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n--- Column Names ---\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\n--- Statistical Summary ---\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n--- Missing Values Count ---\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n--- Duplicate Rows ---\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "print(\"\\n--- Unique Values per Column ---\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f608456",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Convert timestamp to datetime\n",
    "print(\"\\n1. Converting timestamp to datetime...\")\n",
    "df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "df_clean['date'] = df_clean['timestamp'].dt.date\n",
    "df_clean['hour'] = df_clean['timestamp'].dt.hour\n",
    "df_clean['day_of_week'] = df_clean['timestamp'].dt.dayofweek\n",
    "df_clean['day_name'] = df_clean['timestamp'].dt.day_name()\n",
    "print(\"Timestamp converted\")\n",
    "\n",
    "# 2. Handle zero/negative values in price columns\n",
    "print(\"\\n2. Checking for zero/negative prices...\")\n",
    "price_cols = ['price_usd', 'volume_24h', 'market_cap']\n",
    "for col in price_cols:\n",
    "    if col in df_clean.columns:\n",
    "        zero_count = (df_clean[col] == 0).sum()\n",
    "        neg_count = (df_clean[col] < 0).sum()\n",
    "        print(f\"   {col}: {zero_count} zeros, {neg_count} negative values\")\n",
    "        # Replace zeros with NaN for proper handling\n",
    "        df_clean.loc[df_clean[col] == 0, col] = np.nan\n",
    "\n",
    "# 3. Remove duplicates\n",
    "print(\"\\n3. Removing duplicates...\")\n",
    "before_dup = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after_dup = len(df_clean)\n",
    "print(f\"   Removed {before_dup - after_dup} duplicate rows\")\n",
    "\n",
    "# 4. Sort by timestamp and token\n",
    "print(\"\\n4. Sorting data by timestamp and token...\")\n",
    "df_clean = df_clean.sort_values(['token_symbol', 'timestamp']).reset_index(drop=True)\n",
    "print(\"Data sorted\")\n",
    "\n",
    "# 5. Check data quality by source\n",
    "print(\"\\n5. Data quality by source:\")\n",
    "for source in df_clean['source'].unique():\n",
    "    source_data = df_clean[df_clean['source'] == source]\n",
    "    print(f\"\\n   {source}:\")\n",
    "    print(f\"   - Records: {len(source_data)}\")\n",
    "    print(f\"   - Price completeness: {(1 - source_data['price_usd'].isnull().sum()/len(source_data))*100:.2f}%\")\n",
    "    print(f\"   - Tokens covered: {source_data['token_symbol'].nunique()}\")\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e769c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA INTEGRATION - MERGING MULTIPLE SOURCES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Strategy: For each timestamp + token, average the values from different sources\n",
    "print(\"\\n1. Creating integrated dataset with averaged values...\")\n",
    "\n",
    "# Group by timestamp and token, take mean of numeric columns\n",
    "numeric_cols = ['price_usd', 'volume_24h', 'market_cap', 'price_change_1h', \n",
    "                'price_change_24h', 'price_change_7d', 'circulating_supply']\n",
    "\n",
    "# Create aggregation dictionary\n",
    "agg_dict = {col: 'mean' for col in numeric_cols if col in df_clean.columns}\n",
    "agg_dict['source'] = lambda x: ', '.join(x.unique())  # Combine source names\n",
    "agg_dict['token_name'] = 'first'\n",
    "\n",
    "# Round timestamp to nearest 5 minutes for better grouping\n",
    "df_clean['timestamp_rounded'] = df_clean['timestamp'].dt.round('5min')\n",
    "\n",
    "# Aggregate data\n",
    "df_integrated = df_clean.groupby(['timestamp_rounded', 'token_symbol']).agg(agg_dict).reset_index()\n",
    "df_integrated.rename(columns={'timestamp_rounded': 'timestamp'}, inplace=True)\n",
    "\n",
    "print(f\"   Integrated dataset created\")\n",
    "print(f\"   Original records: {len(df_clean)}\")\n",
    "print(f\"   Integrated records: {len(df_integrated)}\")\n",
    "print(f\"   Reduction: {((len(df_clean) - len(df_integrated))/len(df_clean)*100):.2f}%\")\n",
    "\n",
    "# Add back datetime features\n",
    "df_integrated['date'] = df_integrated['timestamp'].dt.date\n",
    "df_integrated['hour'] = df_integrated['timestamp'].dt.hour\n",
    "df_integrated['day_of_week'] = df_integrated['timestamp'].dt.dayofweek\n",
    "df_integrated['day_name'] = df_integrated['timestamp'].dt.day_name()\n",
    "\n",
    "print(\"\\n2. Integrated data sample:\")\n",
    "print(df_integrated.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a0e7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_features = df_integrated.copy()\n",
    "\n",
    "print(\"\\n1. Creating price-based features...\")\n",
    "\n",
    "# Sort by token and timestamp for proper calculations\n",
    "df_features = df_features.sort_values(['token_symbol', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Create lagged features (previous prices)\n",
    "for token in df_features['token_symbol'].unique():\n",
    "    mask = df_features['token_symbol'] == token\n",
    "    \n",
    "    # Price lags\n",
    "    df_features.loc[mask, 'price_lag_1'] = df_features.loc[mask, 'price_usd'].shift(1)\n",
    "    df_features.loc[mask, 'price_lag_2'] = df_features.loc[mask, 'price_usd'].shift(2)\n",
    "    df_features.loc[mask, 'price_lag_3'] = df_features.loc[mask, 'price_usd'].shift(3)\n",
    "    \n",
    "    # Price momentum (rate of change)\n",
    "    df_features.loc[mask, 'price_momentum'] = df_features.loc[mask, 'price_usd'].pct_change()\n",
    "    \n",
    "    # Rolling statistics (3-period window)\n",
    "    df_features.loc[mask, 'price_rolling_mean_3'] = df_features.loc[mask, 'price_usd'].rolling(window=3, min_periods=1).mean()\n",
    "    df_features.loc[mask, 'price_rolling_std_3'] = df_features.loc[mask, 'price_usd'].rolling(window=3, min_periods=1).std()\n",
    "    \n",
    "    # Volume momentum\n",
    "    if 'volume_24h' in df_features.columns:\n",
    "        df_features.loc[mask, 'volume_change'] = df_features.loc[mask, 'volume_24h'].pct_change()\n",
    "\n",
    "print(\"Price-based features created\")\n",
    "\n",
    "print(\"\\n2. Creating technical indicators...\")\n",
    "\n",
    "# Volatility measure\n",
    "df_features['price_volatility'] = df_features.groupby('token_symbol')['price_usd'].transform(\n",
    "    lambda x: x.rolling(window=5, min_periods=1).std()\n",
    ")\n",
    "\n",
    "# Price range\n",
    "df_features['price_range'] = df_features.groupby('token_symbol')['price_usd'].transform(\n",
    "    lambda x: x.rolling(window=5, min_periods=1).max() - x.rolling(window=5, min_periods=1).min()\n",
    ")\n",
    "\n",
    "print(\"Technical indicators created\")\n",
    "\n",
    "print(\"\\n3. Creating target variable...\")\n",
    "\n",
    "# Target: Next period's price change (for classification: up/down)\n",
    "for token in df_features['token_symbol'].unique():\n",
    "    mask = df_features['token_symbol'] == token\n",
    "    df_features.loc[mask, 'next_price'] = df_features.loc[mask, 'price_usd'].shift(-1)\n",
    "    df_features.loc[mask, 'price_change_pct'] = ((df_features.loc[mask, 'next_price'] - df_features.loc[mask, 'price_usd']) \n",
    "                                                   / df_features.loc[mask, 'price_usd'] * 100)\n",
    "\n",
    "# Binary target: 1 if price goes up, 0 if down\n",
    "df_features['target_direction'] = (df_features['price_change_pct'] > 0).astype(int)\n",
    "\n",
    "# Multi-class target: significant up (>2%), moderate up (0-2%), moderate down (0 to -2%), significant down (<-2%)\n",
    "df_features['target_class'] = pd.cut(\n",
    "    df_features['price_change_pct'],\n",
    "    bins=[-np.inf, -2, 0, 2, np.inf],\n",
    "    labels=['significant_down', 'moderate_down', 'moderate_up', 'significant_up']\n",
    ")\n",
    "\n",
    "print(\"   Target variables created\")\n",
    "print(f\"   - next_price: Continuous target\")\n",
    "print(f\"   - price_change_pct: Percentage change\")\n",
    "print(f\"   - target_direction: Binary (0=down, 1=up)\")\n",
    "print(f\"   - target_class: 4 classes\")\n",
    "\n",
    "print(\"\\n4. Target distribution:\")\n",
    "print(df_features['target_direction'].value_counts())\n",
    "print(\"\\nTarget class distribution:\")\n",
    "print(df_features['target_class'].value_counts())\n",
    "\n",
    "print(f\"\\nFeature engineering complete\")\n",
    "print(f\"Total features: {df_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a9836",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HANDLING MISSING VALUES & OUTLIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_processed = df_features.copy()\n",
    "\n",
    "print(\"\\n1. Missing Values Analysis:\")\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df_processed.columns,\n",
    "    'Missing_Count': df_processed.isnull().sum(),\n",
    "    'Missing_Percentage': (df_processed.isnull().sum() / len(df_processed) * 100).round(2)\n",
    "})\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "print(missing_summary)\n",
    "\n",
    "print(\"\\n2. Handling missing values...\")\n",
    "\n",
    "# For numeric columns, use forward fill then backward fill (appropriate for time series)\n",
    "numeric_columns = df_processed.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col] = df_processed.groupby('token_symbol')[col].fillna(method='ffill')\n",
    "        df_processed[col] = df_processed.groupby('token_symbol')[col].fillna(method='bfill')\n",
    "        # If still missing, fill with median\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "\n",
    "print(\"   Missing values handled\")\n",
    "print(f\"   Remaining missing values: {df_processed.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n3. Outlier Detection using IQR method...\")\n",
    "\n",
    "# Detect outliers for key numeric columns\n",
    "outlier_columns = ['price_usd', 'volume_24h', 'market_cap', 'price_change_pct']\n",
    "outlier_summary = []\n",
    "\n",
    "for col in outlier_columns:\n",
    "    if col in df_processed.columns:\n",
    "        Q1 = df_processed[col].quantile(0.25)\n",
    "        Q3 = df_processed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        outliers = ((df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)).sum()\n",
    "        outlier_pct = (outliers / len(df_processed)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outliers': outliers,\n",
    "            'Percentage': f\"{outlier_pct:.2f}%\",\n",
    "            'Lower_Bound': lower_bound,\n",
    "            'Upper_Bound': upper_bound\n",
    "        })\n",
    "        \n",
    "        # Cap outliers (winsorization) instead of removing\n",
    "        df_processed[col] = np.clip(df_processed[col], lower_bound, upper_bound)\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df)\n",
    "print(\"\\nOutliers capped using winsorization (3*IQR)\")\n",
    "\n",
    "# Remove rows with missing target (last few rows due to shift)\n",
    "df_processed = df_processed.dropna(subset=['next_price', 'target_direction'])\n",
    "\n",
    "print(f\"\\nFinal processed dataset shape: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955c008",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA STANDARDIZATION & NORMALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Separate features and targets\n",
    "feature_cols = [col for col in df_processed.columns if col not in \n",
    "                ['timestamp', 'source', 'token_symbol', 'token_name', 'date', 'day_name',\n",
    "                 'next_price', 'price_change_pct', 'target_direction', 'target_class']]\n",
    "\n",
    "print(f\"\\nFeatures to standardize: {len(feature_cols)}\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Create a copy for standardization\n",
    "df_standardized = df_processed.copy()\n",
    "\n",
    "print(\"\\n1. Applying Z-Score Standardization (StandardScaler)...\")\n",
    "scaler_standard = StandardScaler()\n",
    "df_standardized[feature_cols] = scaler_standard.fit_transform(df_processed[feature_cols])\n",
    "print(\"   Z-Score standardization complete\")\n",
    "print(f\"   Mean: {df_standardized[feature_cols].mean().mean():.6f}\")\n",
    "print(f\"   Std: {df_standardized[feature_cols].std().mean():.6f}\")\n",
    "\n",
    "print(\"\\n2. Creating MinMax normalized version (0-1 scale)...\")\n",
    "df_normalized = df_processed.copy()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_normalized[feature_cols] = scaler_minmax.fit_transform(df_processed[feature_cols])\n",
    "print(\"   MinMax normalization complete\")\n",
    "print(f\"   Min: {df_normalized[feature_cols].min().min():.6f}\")\n",
    "print(f\"   Max: {df_normalized[feature_cols].max().max():.6f}\")\n",
    "\n",
    "print(\"\\n3. Creating Robust Scaler version (resistant to outliers)...\")\n",
    "df_robust = df_processed.copy()\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust[feature_cols] = scaler_robust.fit_transform(df_processed[feature_cols])\n",
    "print(\" Robust scaling complete\")\n",
    "\n",
    "print(\"\\n4. Comparison of scaling methods on sample feature (price_usd):\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': df_processed['price_usd'].head(10),\n",
    "    'StandardScaler': df_standardized['price_usd'].head(10),\n",
    "    'MinMaxScaler': df_normalized['price_usd'].head(10),\n",
    "    'RobustScaler': df_robust['price_usd'].head(10)\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\nAll scaling methods applied successfully!\")\n",
    "print(\"\\nRecommendation: Use StandardScaler for most ML models (Random Forest, XGBoost)\")\n",
    "print(\"               Use MinMaxScaler for neural networks\")\n",
    "print(\"               Use RobustScaler if outliers are still a concern\")\n",
    "\n",
    "# We'll use StandardScaler version for further analysis\n",
    "df_final = df_standardized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2a185f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS (EDA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total Records: {len(df_final)}\")\n",
    "print(f\"Total Features: {len(feature_cols)}\")\n",
    "print(f\"Date Range: {df_final['timestamp'].min()} to {df_final['timestamp'].max()}\")\n",
    "print(f\"Total Trading Days: {df_final['date'].nunique()}\")\n",
    "print(f\"Tokens Analyzed: {df_final['token_symbol'].nunique()}\")\n",
    "print(f\"Token List: {', '.join(df_final['token_symbol'].unique())}\")\n",
    "\n",
    "print(\"\\n2. TARGET VARIABLE ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nBinary Target (Price Direction):\")\n",
    "target_counts = df_final['target_direction'].value_counts()\n",
    "print(f\"Up (1): {target_counts.get(1, 0)} ({target_counts.get(1, 0)/len(df_final)*100:.2f}%)\")\n",
    "print(f\"Down (0): {target_counts.get(0, 0)} ({target_counts.get(0, 0)/len(df_final)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nMulti-class Target Distribution:\")\n",
    "print(df_final['target_class'].value_counts())\n",
    "\n",
    "print(\"\\n3. TOKEN-WISE STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "token_stats = df_final.groupby('token_symbol').agg({\n",
    "    'price_usd': ['mean', 'std', 'min', 'max'],\n",
    "    'volume_24h': 'mean',\n",
    "    'market_cap': 'mean',\n",
    "    'target_direction': lambda x: (x == 1).sum() / len(x) * 100  # % times price went up\n",
    "}).round(4)\n",
    "token_stats.columns = ['Price_Mean', 'Price_Std', 'Price_Min', 'Price_Max', \n",
    "                       'Avg_Volume', 'Avg_MarketCap', 'Win_Rate_%']\n",
    "print(token_stats)\n",
    "\n",
    "print(\"\\n4. CORRELATION ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Select numeric features for correlation\n",
    "numeric_features = df_final[feature_cols].select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_features.corr()\n",
    "\n",
    "print(\"\\nTop 10 Positive Correlations with Price USD:\")\n",
    "price_corr = correlation_matrix['price_usd'].sort_values(ascending=False)[1:11]\n",
    "print(price_corr)\n",
    "\n",
    "print(\"\\nTop 10 Negative Correlations with Price USD:\")\n",
    "price_corr_neg = correlation_matrix['price_usd'].sort_values()[0:10]\n",
    "print(price_corr_neg)\n",
    "\n",
    "print(\"\\n5. FEATURE IMPORTANCE (Correlation with Target)\")\n",
    "print(\"-\" * 80)\n",
    "# Get correlations with binary target\n",
    "target_correlations = df_final[feature_cols].corrwith(df_final['target_direction']).abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 15 Features Correlated with Price Direction:\")\n",
    "print(target_correlations.head(15))\n",
    "\n",
    "print(\"\\n6. TIME-BASED ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\nPrice movement by day of week:\")\n",
    "day_analysis = df_final.groupby('day_name')['target_direction'].agg(['mean', 'count'])\n",
    "day_analysis.columns = ['Win_Rate', 'Count']\n",
    "day_analysis['Win_Rate'] = (day_analysis['Win_Rate'] * 100).round(2)\n",
    "print(day_analysis)\n",
    "\n",
    "print(\"\\nPrice movement by hour:\")\n",
    "hour_analysis = df_final.groupby('hour')['target_direction'].agg(['mean', 'count'])\n",
    "hour_analysis.columns = ['Win_Rate', 'Count']\n",
    "hour_analysis['Win_Rate'] = (hour_analysis['Win_Rate'] * 100).round(2)\n",
    "print(hour_analysis.head(10))\n",
    "\n",
    "print(\"\\n7. VOLATILITY ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "volatility_stats = df_final.groupby('token_symbol')['price_volatility'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(volatility_stats)\n",
    "\n",
    "print(\"\\n8. MISSING DATA FINAL CHECK\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total missing values in final dataset: {df_final.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - Ready for Visualization!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4989600",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 1: PRICE TRENDS OVER TIME\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Go back to non-standardized data for visualization\n",
    "df_viz = df_processed.copy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 12))\n",
    "fig.suptitle('Price Trends by Token (USD)', fontsize=16, fontweight='bold')\n",
    "\n",
    "tokens = df_viz['token_symbol'].unique()\n",
    "for idx, token in enumerate(tokens):\n",
    "    if idx < 9:  # Plot up to 9 tokens\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        token_data = df_viz[df_viz['token_symbol'] == token].sort_values('timestamp')\n",
    "        \n",
    "        ax.plot(token_data['timestamp'], token_data['price_usd'], linewidth=2, color='blue', alpha=0.7)\n",
    "        ax.fill_between(token_data['timestamp'], token_data['price_usd'], alpha=0.3)\n",
    "        ax.set_title(f'{token}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Price (USD)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Price trends plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cb654",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 2: CORRELATION HEATMAP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select top correlated features for cleaner visualization\n",
    "top_features = target_correlations.head(15).index.tolist()\n",
    "top_features.append('target_direction')\n",
    "\n",
    "correlation_subset = df_final[top_features].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_subset, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Heatmap - Top 15 Features vs Target', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation heatmap plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db49b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 3: FEATURE DISTRIBUTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot distributions of key features\n",
    "key_features = ['price_usd', 'volume_24h', 'market_cap', 'price_change_24h', \n",
    "                'price_volatility', 'price_momentum']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Distribution of Key Features', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in df_viz.columns:\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        \n",
    "        # Histogram with KDE\n",
    "        df_viz[feature].hist(bins=50, ax=ax, alpha=0.6, color='skyblue', edgecolor='black')\n",
    "        ax2 = ax.twinx()\n",
    "        df_viz[feature].plot(kind='kde', ax=ax2, color='red', linewidth=2)\n",
    "        \n",
    "        ax.set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(feature)\n",
    "        ax.set_ylabel('Frequency', color='blue')\n",
    "        ax2.set_ylabel('Density', color='red')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Distribution plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d06039",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 4: BOX PLOTS FOR OUTLIER DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Box Plots - Outlier Detection', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in df_viz.columns:\n",
    "        ax = axes[idx // 3, idx % 3]\n",
    "        df_viz.boxplot(column=feature, by='token_symbol', ax=ax, rot=90)\n",
    "        ax.set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Token')\n",
    "        ax.set_ylabel(feature)\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Box plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9e1fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 5: TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Target Variable Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Binary target distribution\n",
    "ax1 = axes[0, 0]\n",
    "target_counts = df_viz['target_direction'].value_counts()\n",
    "colors = ['#ff6b6b', '#51cf66']\n",
    "ax1.bar(target_counts.index, target_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Binary Target Distribution (0=Down, 1=Up)', fontweight='bold')\n",
    "ax1.set_xlabel('Direction')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_xticklabels(['Down', 'Up'])\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    ax1.text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Multi-class target distribution\n",
    "ax2 = axes[0, 1]\n",
    "class_counts = df_viz['target_class'].value_counts()\n",
    "colors2 = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "ax2.bar(range(len(class_counts)), class_counts.values, color=colors2, alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Multi-class Target Distribution', fontweight='bold')\n",
    "ax2.set_xlabel('Class')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xticks(range(len(class_counts)))\n",
    "ax2.set_xticklabels(class_counts.index, rotation=45, ha='right')\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    ax2.text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Target distribution by token\n",
    "ax3 = axes[1, 0]\n",
    "token_target = df_viz.groupby('token_symbol')['target_direction'].mean() * 100\n",
    "token_target.sort_values(ascending=False).plot(kind='barh', ax=ax3, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax3.set_title('Win Rate by Token (%)', fontweight='bold')\n",
    "ax3.set_xlabel('Win Rate (%)')\n",
    "ax3.set_ylabel('Token')\n",
    "ax3.axvline(x=50, color='red', linestyle='--', linewidth=2, label='50% baseline')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Price change percentage distribution\n",
    "ax4 = axes[1, 1]\n",
    "df_viz['price_change_pct'].hist(bins=50, ax=ax4, alpha=0.7, color='purple', edgecolor='black')\n",
    "ax4.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No change')\n",
    "ax4.set_title('Distribution of Price Changes (%)', fontweight='bold')\n",
    "ax4.set_xlabel('Price Change (%)')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Target variable analysis plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449849b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 6: TIME SERIES PATTERNS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Time Series Patterns Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Average price by day of week\n",
    "ax1 = axes[0, 0]\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_price = df_viz.groupby('day_name')['price_usd'].mean().reindex(day_order)\n",
    "day_price.plot(kind='bar', ax=ax1, color='teal', alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Average Price by Day of Week', fontweight='bold')\n",
    "ax1.set_xlabel('Day')\n",
    "ax1.set_ylabel('Average Price (USD)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Win rate by day of week\n",
    "ax2 = axes[0, 1]\n",
    "day_win = df_viz.groupby('day_name')['target_direction'].mean().reindex(day_order) * 100\n",
    "day_win.plot(kind='bar', ax=ax2, color='coral', alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=50, color='red', linestyle='--', linewidth=2, label='50% baseline')\n",
    "ax2.set_title('Win Rate by Day of Week', fontweight='bold')\n",
    "ax2.set_xlabel('Day')\n",
    "ax2.set_ylabel('Win Rate (%)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Average price by hour\n",
    "ax3 = axes[1, 0]\n",
    "hour_price = df_viz.groupby('hour')['price_usd'].mean()\n",
    "ax3.plot(hour_price.index, hour_price.values, marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "ax3.fill_between(hour_price.index, hour_price.values, alpha=0.3)\n",
    "ax3.set_title('Average Price by Hour of Day', fontweight='bold')\n",
    "ax3.set_xlabel('Hour (24h format)')\n",
    "ax3.set_ylabel('Average Price (USD)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xticks(range(0, 24, 2))\n",
    "\n",
    "# 4. Trading volume over time\n",
    "ax4 = axes[1, 1]\n",
    "volume_by_token = df_viz.groupby(['timestamp', 'token_symbol'])['volume_24h'].sum().reset_index()\n",
    "for token in volume_by_token['token_symbol'].unique()[:5]:  # Top 5 tokens\n",
    "    token_vol = volume_by_token[volume_by_token['token_symbol'] == token]\n",
    "    ax4.plot(token_vol['timestamp'], token_vol['volume_24h'], label=token, linewidth=2, alpha=0.7)\n",
    "ax4.set_title('24h Trading Volume by Token', fontweight='bold')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_ylabel('Volume (USD)')\n",
    "ax4.legend(loc='best')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Time series pattern plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85977064",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 7: ADVANCED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Advanced Analysis Plots', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Volatility vs Returns scatter\n",
    "ax1 = axes[0, 0]\n",
    "for token in df_viz['token_symbol'].unique()[:7]:\n",
    "    token_data = df_viz[df_viz['token_symbol'] == token]\n",
    "    ax1.scatter(token_data['price_volatility'], token_data['price_change_pct'], \n",
    "               label=token, alpha=0.6, s=50)\n",
    "ax1.set_title('Volatility vs Price Change', fontweight='bold')\n",
    "ax1.set_xlabel('Price Volatility')\n",
    "ax1.set_ylabel('Price Change (%)')\n",
    "ax1.legend(loc='best', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# 2. Volume vs Price correlation\n",
    "ax2 = axes[0, 1]\n",
    "for token in df_viz['token_symbol'].unique()[:7]:\n",
    "    token_data = df_viz[df_viz['token_symbol'] == token]\n",
    "    ax2.scatter(token_data['volume_24h'], token_data['price_usd'], \n",
    "               label=token, alpha=0.6, s=50)\n",
    "ax2.set_title('Volume vs Price Relationship', fontweight='bold')\n",
    "ax2.set_xlabel('24h Volume (USD)')\n",
    "ax2.set_ylabel('Price (USD)')\n",
    "ax2.legend(loc='best', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Rolling mean comparison\n",
    "ax3 = axes[1, 0]\n",
    "sample_token = df_viz['token_symbol'].unique()[0]  # Pick first token\n",
    "sample_data = df_viz[df_viz['token_symbol'] == sample_token].sort_values('timestamp')\n",
    "ax3.plot(sample_data['timestamp'], sample_data['price_usd'], label='Actual Price', linewidth=2, alpha=0.7)\n",
    "ax3.plot(sample_data['timestamp'], sample_data['price_rolling_mean_3'], \n",
    "        label='3-Period Rolling Mean', linewidth=2, linestyle='--')\n",
    "ax3.set_title(f'Price vs Rolling Mean - {sample_token}', fontweight='bold')\n",
    "ax3.set_xlabel('Time')\n",
    "ax3.set_ylabel('Price (USD)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Market cap distribution\n",
    "ax4 = axes[1, 1]\n",
    "market_cap_by_token = df_viz.groupby('token_symbol')['market_cap'].mean().sort_values(ascending=False)\n",
    "colors_map = plt.cm.viridis(np.linspace(0, 1, len(market_cap_by_token)))\n",
    "ax4.barh(range(len(market_cap_by_token)), market_cap_by_token.values, color=colors_map, edgecolor='black')\n",
    "ax4.set_yticks(range(len(market_cap_by_token)))\n",
    "ax4.set_yticklabels(market_cap_by_token.index)\n",
    "ax4.set_title('Average Market Cap by Token', fontweight='bold')\n",
    "ax4.set_xlabel('Market Cap (USD)')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Advanced analysis plots created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf31574",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION 8: FEATURE IMPORTANCE PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot top features correlated with target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# 1. Top positive correlations\n",
    "ax1 = axes[0]\n",
    "top_positive = target_correlations.head(15)\n",
    "top_positive.plot(kind='barh', ax=ax1, color='green', alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Top 15 Features - Correlation with Target', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('Absolute Correlation')\n",
    "ax1.set_ylabel('Feature')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Correlation strength distribution\n",
    "ax2 = axes[1]\n",
    "all_correlations = df_final[feature_cols].corrwith(df_final['target_direction']).abs()\n",
    "ax2.hist(all_correlations, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax2.axvline(x=all_correlations.mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {all_correlations.mean():.3f}')\n",
    "ax2.set_title('Distribution of Feature Correlations', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Absolute Correlation with Target')\n",
    "ax2.set_ylabel('Number of Features')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature importance visualization created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405366d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL DATASET SUMMARY & PREPARATION FOR ML\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. FINAL DATASET STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Total Records: {len(df_final)}\")\n",
    "print(f\"Total Features: {len(feature_cols)}\")\n",
    "print(f\"Training-ready samples: {len(df_final.dropna())}\")\n",
    "print(f\"Missing values: {df_final.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n2. FEATURE CATEGORIES\")\n",
    "print(\"-\" * 80)\n",
    "price_features = [f for f in feature_cols if 'price' in f.lower()]\n",
    "volume_features = [f for f in feature_cols if 'volume' in f.lower()]\n",
    "time_features = [f for f in feature_cols if any(x in f.lower() for x in ['hour', 'day', 'week'])]\n",
    "technical_features = [f for f in feature_cols if any(x in f.lower() for x in ['volatility', 'momentum', 'rolling'])]\n",
    "\n",
    "print(f\"Price-related features: {len(price_features)}\")\n",
    "print(f\"Volume-related features: {len(volume_features)}\")\n",
    "print(f\"Time-based features: {len(time_features)}\")\n",
    "print(f\"Technical indicators: {len(technical_features)}\")\n",
    "\n",
    "print(\"\\n3. TARGET VARIABLE SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Binary Target (target_direction):\")\n",
    "print(f\"  - Class 0 (Down): {(df_final['target_direction'] == 0).sum()}\")\n",
    "print(f\"  - Class 1 (Up): {(df_final['target_direction'] == 1).sum()}\")\n",
    "print(f\"  - Balance ratio: {min(target_counts) / max(target_counts):.2f}\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDED ML MODELS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Based on the data characteristics:\")\n",
    "print(\"  Random Forest Classifier - Handles non-linear relationships well\")\n",
    "print(\"  XGBoost/LightGBM - Excellent for tabular data, fast training\")\n",
    "print(\"  Gradient Boosting - Good performance on imbalanced data\")\n",
    "print(\"  Neural Networks - May need more data, longer training\")\n",
    "print(\"  Logistic Regression - Simple baseline, may underfit\")\n",
    "\n",
    "print(\"\\n5. DATA QUALITY CHECKS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"No missing values in features\")\n",
    "print(\"No duplicate records\")\n",
    "print(\"Outliers handled via winsorization\")\n",
    "print(\"Features standardized (Z-score)\")\n",
    "print(\"Target variable balanced enough for training\")\n",
    "\n",
    "print(\"\\n6. TRAIN/TEST SPLIT RECOMMENDATION\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Recommended split strategy:\")\n",
    "print(\"  - Time-based split (80% train, 20% test)\")\n",
    "print(\"  - Cross-validation: 5-fold TimeSeriesSplit\")\n",
    "print(\"  - Avoid data leakage by respecting temporal order\")\n",
    "\n",
    "print(\"\\n7. EXPORTING PREPARED DATASETS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Export different versions\n",
    "output_files = {\n",
    "    'processed': df_processed,      # Non-standardized\n",
    "    'standardized': df_final,       # Z-score standardized\n",
    "    'normalized': df_normalized,    # MinMax scaled\n",
    "    'robust': df_robust            # Robust scaled\n",
    "}\n",
    "\n",
    "for name, data in output_files.items():\n",
    "    filename = f'./crypto_data_{name}.csv'\n",
    "    data.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename} ({len(data)} rows, {len(data.columns)} columns)\")\n",
    "\n",
    "print(\"\\n8. FEATURE LIST FOR ML TRAINING\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Copy this list for your ML model:\")\n",
    "print(f\"FEATURE_COLS = {feature_cols}\")\n",
    "\n",
    "print(\"\\n9. READY FOR MODEL TRAINING\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Split data using TimeSeriesSplit\")\n",
    "print(\"  2. Train Random Forest / XGBoost models\")\n",
    "print(\"  3. Evaluate using SMAPE, MAE, Accuracy, F1-Score\")\n",
    "print(\"  4. Perform hyperparameter tuning (5-fold CV)\")\n",
    "print(\"  5. Select best model for deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PROCESSING & EDA COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"   - Collected data from 2 sources (CoinGecko + CoinMarketCap)\")\n",
    "print(f\"   - Cleaned and processed {len(df_final)} records\")\n",
    "print(f\"   - Created {len(feature_cols)} features\")\n",
    "print(f\"   - Generated comprehensive visualizations\")\n",
    "print(f\"   - Prepared 4 different scaled versions\")\n",
    "print(f\"   - Ready for ML model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c53828",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ML PREPROCESSING HELPER CODE (Use before training)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# This cell prepares X and y for ML training\n",
    "# Use the standardized dataset\n",
    "X = df_final[feature_cols].copy()\n",
    "y_binary = df_final['target_direction'].copy()\n",
    "y_multiclass = df_final['target_class'].copy()\n",
    "y_regression = df_final['price_change_pct'].copy()\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Binary target (y_binary) shape: {y_binary.shape}\")\n",
    "print(f\"Multiclass target (y_multiclass) shape: {y_multiclass.shape}\")\n",
    "print(f\"Regression target (y_regression) shape: {y_regression.shape}\")\n",
    "\n",
    "print(\"\\nâœ… X and y variables ready for model training\")\n",
    "print(\"\\nQuick verification:\")\n",
    "print(f\"  - No NaN in X: {X.isnull().sum().sum() == 0}\")\n",
    "print(f\"  - No NaN in y_binary: {y_binary.isnull().sum() == 0}\")\n",
    "print(f\"  - Feature types: {X.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF DATA PROCESSING & VISUALIZATION\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
